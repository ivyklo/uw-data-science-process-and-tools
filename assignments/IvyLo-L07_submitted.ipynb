{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "\n",
    "1. use pandas read_csv with sep='\\t' to read in the following 2 files available from the us naval academy:\n",
    "- url = 'https://www.usna.edu/Users/cs/nchamber/data/twitter/keyword-tweets.txt'\n",
    "- url = 'https://www.usna.edu/Users/cs/nchamber/data/twitter/general-tweets.txt'\n",
    "<br/> <span style=\"color:red\" float:right>[1 point]</span>\n",
    "\n",
    "2. concatenate these 2 data sets into a single data frame called LabeledTweets that has 2 columns, named Sentiment and Tweet <span style=\"color:red\" float:right>[1 point]</span>\n",
    "\n",
    "3. replace sentiment labels 'POLIT': 1, 'NOT': 0; <span style=\"color:red\" float:right>[0 point]</span>\n",
    "\n",
    "4. clean the tweets\n",
    "   1. remove all tokens that contain a \"@\". Remove the whole token, not just the character.\n",
    "   2. remove all tokens that contain \"http\". Remove the whole token, not just the characters.\n",
    "   3. **replace** (not remove) all punctuation marks with a space (\" \")\n",
    "   4. **replace** all numbers with a space\n",
    "   5. **replace** all non ascii characters with a space\n",
    "   7. convert all characters to lowercase\n",
    "   8. strip extra whitespaces\n",
    "   9. lemmatize tokens\n",
    "   9. No need to remove stopwords because TfidfVectorizer will take care of that\n",
    "<br/><span style=\"color:red\" float:right>[9 point]</span>\n",
    "\n",
    "5. Use TfidfVectorizer from sklearn to prepare the data for machine learning.  Use max_features = 50;  <span style=\"color:red\" float:right>[2 point]</span>\n",
    "\n",
    "6. Use sklearn LogisticRegression to train a model on the  results on 75% of the data. <span style=\"color:red\" float:right>[1 point]</span>\n",
    "\n",
    "7. Determine the accuracy of your model on the training data and the test data.   Determine the baseline accuracy. <span style=\"color:red\" float:right>[1 point]</span>\n",
    "\n",
    "8. Repeat steps 5, 6, and 7  with TfidfVectorizer max_features set to 5, 500, 5000, 50000 and discuss your accuracies. <span style=\"color:red\" float:right>[2 point]</span>\n",
    "\n",
    "# End of assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cProfile\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.sparse import coo_matrix # this is the sparse matrix format discussed in lecture\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "\n",
    "import nltk\n",
    "# # A one-time requirement for these four downloads:\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('corpus')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword shape: (2004, 2)\n",
      "   col_1                                               text\n",
      "0  POLIT  Global Voices Online Â» Alex Castro: A liberal...\n",
      "1  POLIT  Do the Conservatives Have a Death Wish? http:/...\n",
      "2    NOT  @MMFlint I've seen all of your movies and Capi...\n",
      "3  POLIT  RT @AllianceAlert: * House Dems ask for civili...\n",
      "4  POLIT  RT @AdamSmithInst Quote of the week: My politi...\n",
      "general_df: (2000, 2)\n",
      "  col_1                                               text\n",
      "0   NOT  Bumping dj sefs mixtape nowww this is my music...\n",
      "1   NOT  #ieroween THE STORY OF IEROWEEN! THE VIDEO ->>...\n",
      "2   NOT  trick or treating at the mall today; ZOO! last...\n",
      "3   NOT  @Ussk81 PMSL!!! I try not to stare but I can't...\n",
      "4   NOT  @Sc0rpi0n676 btw - is there a remote chance i ...\n"
     ]
    }
   ],
   "source": [
    "# 1. use pandas read_csv with sep='\\t' to read in the following 2 files available from the us naval academy:\n",
    "url_keyword = 'https://www.usna.edu/Users/cs/nchamber/data/twitter/keyword-tweets.txt'\n",
    "url_general = 'https://www.usna.edu/Users/cs/nchamber/data/twitter/general-tweets.txt'\n",
    "\n",
    "# read into dataframes\n",
    "keyword_df = pd.read_csv(url_keyword, sep ='\\t', header = None, names = ['col_1', 'text'])\n",
    "general_df = pd.read_csv(url_general, sep ='\\t', header = None, names = ['col_1', 'text'])\n",
    "\n",
    "# check\n",
    "print('Keyword shape:', keyword_df.shape)\n",
    "print(keyword_df.head())\n",
    "\n",
    "print('general_df:', general_df.shape)\n",
    "print(general_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sentiment                                              Tweet\n",
      "0     POLIT  Global Voices Online Â» Alex Castro: A liberal...\n",
      "1     POLIT  Do the Conservatives Have a Death Wish? http:/...\n",
      "2       NOT  @MMFlint I've seen all of your movies and Capi...\n",
      "3     POLIT  RT @AllianceAlert: * House Dems ask for civili...\n",
      "4     POLIT  RT @AdamSmithInst Quote of the week: My politi...\n",
      "(4004, 2)\n",
      "Sentiment\n",
      "NOT      2285\n",
      "POLIT    1719\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 2. concatenate these 2 data sets into a single data frame called LabeledTweets that has 2 columns, named Sentiment and Tweet [1 point]\n",
    "\n",
    "# Concatenate the two DataFrames\n",
    "LabeledTweets = pd.concat([keyword_df, general_df], axis=0, ignore_index=True)\n",
    "\n",
    "# Rename columns\n",
    "LabeledTweets.columns = ['Sentiment', 'Tweet']\n",
    "\n",
    "\n",
    "# check\n",
    "print(LabeledTweets.head())\n",
    "print(LabeledTweets.shape)\n",
    "print(LabeledTweets['Sentiment'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Sentiment                                              Tweet\n",
      "0          1  Global Voices Online Â» Alex Castro: A liberal...\n",
      "1          1  Do the Conservatives Have a Death Wish? http:/...\n",
      "2          0  @MMFlint I've seen all of your movies and Capi...\n",
      "3          1  RT @AllianceAlert: * House Dems ask for civili...\n",
      "4          1  RT @AdamSmithInst Quote of the week: My politi...\n",
      "Sentiment\n",
      "0    2285\n",
      "1    1719\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 3. replace sentiment labels 'POLIT': 1, 'NOT': 0; [0 point]\n",
    "\n",
    "# create a mapping dictionary\n",
    "sentiment_mapping = {\n",
    "    'POLIT': 1, # political \n",
    "    'NOT': 0 # non-political\n",
    "}\n",
    "\n",
    "# apply mapping to the sentiment columns \n",
    "LabeledTweets['Sentiment'] = LabeledTweets['Sentiment'].map(sentiment_mapping)\n",
    "\n",
    "# check\n",
    "print(LabeledTweets.head())\n",
    "print(LabeledTweets['Sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    global voice online alex castro a liberal libe...\n",
      "1                do the conservative have a death wish\n",
      "2    i ve seen all of your movie and capitalism is ...\n",
      "3    rt house dems ask for civility at town hall an...\n",
      "4    rt quote of the week my political opinion lean...\n",
      "Name: Tweet, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 4. clean the tweets\n",
    "\n",
    "# remove all tokens that contain a \"@\". Remove the whole token, not just the character.\n",
    "# remove all tokens that contain \"http\". Remove the whole token, not just the characters.\n",
    "# replace (not remove) all punctuation marks with a space (\" \")\n",
    "# replace all numbers with a space\n",
    "# replace all non ascii characters with a space\n",
    "# convert all characters to lowercase\n",
    "# strip extra whitespaces\n",
    "# lemmatize tokens\n",
    "# No need to remove stopwords because TfidfVectorizer will take care of that\n",
    "\n",
    "# create the clean function \n",
    "\n",
    "def clean(text, list_of_steps):\n",
    "    \n",
    "    for step in list_of_steps:\n",
    "        # remove all tokens that contain a \"@\". Remove the whole token, not just the character.\n",
    "        if step == 'remove_@':             \n",
    "            text = ' '.join([token for token in text.split() if '@' not in token])\n",
    "        \n",
    "        # remove all tokens that contain \"http\". Remove the whole token, not just the characters.\n",
    "        elif step == 'remove_http':         \n",
    "            text = ' '.join([token for token in text.split() if 'http' not in token]) \n",
    "        \n",
    "        # replace (not remove) all punctuation marks with a space (\" \")\n",
    "        elif step == 'replace_punctuation': \n",
    "            punc = set(string.punctuation)\n",
    "            text = ''.join([char if char not in punc else ' ' for char in text])\n",
    "        \n",
    "        # replace all numbers with a space\n",
    "        elif step == 'replace_numbers':\n",
    "            text = ''.join([char if not char.isdigit() else ' ' for char in text])\n",
    "        \n",
    "        # replace all non ascii characters with a space\n",
    "        elif step == 'replace_non_ascii':    \n",
    "            text = ''.join([char if ord(char) < 128 else ' ' for char in text])\n",
    "        \n",
    "        # convert all characters to lowercase\n",
    "        elif step == 'lowercase':            \n",
    "            text = text.lower()\n",
    "          \n",
    "        # lemmatize tokens\n",
    "        elif step == 'lemmatize':\n",
    "            lmtzr = WordNetLemmatizer()\n",
    "            word_list = text.split(' ')\n",
    "            stemmed_words = [lmtzr.lemmatize(word) for word in word_list]\n",
    "            text = ' '.join(stemmed_words)\n",
    "        \n",
    "        # strip extra whitespaces\n",
    "        elif step == 'strip_whitespace':\n",
    "            text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "step_list = ['remove_@', 'remove_http', 'replace_punctuation', 'replace_numbers', 'replace_non_ascii', 'lowercase', 'lemmatize', 'strip_whitespace']\n",
    "\n",
    "\n",
    "# apply the clean function to the 'Tweet' column\n",
    "cleaned_tweets = LabeledTweets['Tweet'].map(lambda x: clean(x, step_list))\n",
    "\n",
    "# Save back to DataFrame\n",
    "LabeledTweets['clean_tweet'] = cleaned_tweets\n",
    "\n",
    "# check\n",
    "print(cleaned_tweets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   afghanistan  better    care     com  congress  conservative     day  \\\n",
      "0      0.00000 0.00000 0.00000 0.00000   0.00000       0.00000 0.00000   \n",
      "1      0.00000 0.00000 0.00000 0.00000   0.00000       1.00000 0.00000   \n",
      "2      0.00000 0.00000 0.00000 0.00000   0.00000       0.00000 0.00000   \n",
      "3      0.00000 0.00000 0.00000 0.00000   0.00000       0.00000 0.00000   \n",
      "4      0.00000 0.00000 0.00000 0.00000   0.00000       0.00000 0.00000   \n",
      "\n",
      "      did     don  economy  ...  stimulus    tcot   think    time   today  \\\n",
      "0 0.00000 0.00000  0.00000  ...   0.00000 0.00000 0.00000 0.00000 0.00000   \n",
      "1 0.00000 0.00000  0.00000  ...   0.00000 0.00000 0.00000 0.00000 0.00000   \n",
      "2 0.00000 0.00000  0.00000  ...   0.00000 0.00000 0.00000 0.00000 0.00000   \n",
      "3 0.00000 0.00000  0.00000  ...   0.00000 0.00000 0.00000 0.00000 0.00000   \n",
      "4 0.00000 0.00000  0.00000  ...   0.00000 0.00000 0.00000 0.00000 0.00000   \n",
      "\n",
      "    video      wa    want     way    work  \n",
      "0 0.00000 0.00000 0.00000 0.00000 0.00000  \n",
      "1 0.00000 0.00000 0.00000 0.00000 0.00000  \n",
      "2 0.00000 0.00000 0.00000 0.00000 0.68255  \n",
      "3 0.00000 0.00000 0.00000 0.00000 0.00000  \n",
      "4 0.00000 0.00000 0.00000 0.00000 0.00000  \n",
      "\n",
      "[5 rows x 50 columns]\n"
     ]
    }
   ],
   "source": [
    "# 5. Use TfidfVectorizer from sklearn to prepare the data for machine learning. Use max_features = 50\n",
    "\n",
    "# initialize the vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=50, stop_words = 'english')\n",
    "\n",
    "# fit and transform the cleaned tweets\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(LabeledTweets['clean_tweet'])\n",
    "\n",
    "# convert to array or DataFrame to check\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "print(tfidf_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Use sklearn LogisticRegression to train a model on the results on 75% of the data.\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# define y\n",
    "y = LabeledTweets['Sentiment']\n",
    "\n",
    "# split train test data by 75% train, 25% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    tfidf_matrix, y, test_size=0.25, random_state=42, stratify=y  # set a seed 42 for consistent results for each run \n",
    ")                                                                 # stratify = y ensures class distribution in train and test sets is same as original data.\n",
    "\n",
    "# initialize training on logistic regression\n",
    "model1 = LogisticRegression(max_iter=1000)  # max_iter increased to ensure convergence\n",
    "model1.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred = model1.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8861\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.92      0.90       571\n",
      "           1       0.89      0.84      0.86       430\n",
      "\n",
      "    accuracy                           0.89      1001\n",
      "   macro avg       0.89      0.88      0.88      1001\n",
      "weighted avg       0.89      0.89      0.89      1001\n",
      "\n",
      "Baseline accuracy: 0.5707\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7. Determine the accuracy of your model on the training data and the test data. Determine the baseline accuracy. \n",
    "# evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}\\n')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# baseline accuracy\n",
    "# It tells how well a model would perform without learning anything\n",
    "baseline_accuracy = y.value_counts().max() / len(y)\n",
    "print(f'Baseline accuracy: {baseline_accuracy:.4f}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of this model is 0.8861. Baseline accuracy is 0.5707. Our model 1 is more accurate than baseline accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 with feature = 50, Accuracy: 0.8861\n",
      "\n",
      "Model 2 with feature = 5, Accuracy: 0.7413\n",
      "\n",
      "Model 3 with feature = 500, Accuracy: 0.8931\n",
      "\n",
      "Model 4 with feature = 5000, Accuracy: 0.8691\n",
      "\n",
      "Model 5 with feature = 50000, Accuracy: 0.8691\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8. Repeat steps 5, 6, and 7 with TfidfVectorizer max_features set to 5, 500, 5000, 50000 and discuss your accuracies.\n",
    "\n",
    "# model 1, feature = 50\n",
    "y_pred = model1.predict(X_test)\n",
    "model1_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Model 1 with feature = 50, Accuracy: {model1_accuracy:.4f}\\n')\n",
    "\n",
    "# model 2, feature = 5\n",
    "# initialize the vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5, stop_words = 'english')\n",
    "# fit and transform the cleaned tweets\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(LabeledTweets['clean_tweet'])\n",
    "# define y\n",
    "y = LabeledTweets['Sentiment']\n",
    "\n",
    "# split train test data by 75% train, 25% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    tfidf_matrix, y, test_size=0.25, random_state=42, stratify=y  # set a seed 42 for consistent results for each run \n",
    ")                                                                 # stratify = y ensures class distribution in train and test sets is same as original data.\n",
    "\n",
    "# initialize training on logistic regression\n",
    "model2 = LogisticRegression(max_iter=1000)  # max_iter increased to ensure convergence\n",
    "model2.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred = model2.predict(X_test)\n",
    "# accuracy\n",
    "model2_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Model 2 with feature = 5, Accuracy: {model2_accuracy:.4f}\\n')\n",
    "\n",
    "\n",
    "# model 3, feature = 500\n",
    "# initialize the vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=500, stop_words = 'english')\n",
    "# fit and transform the cleaned tweets\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(LabeledTweets['clean_tweet'])\n",
    "# define y\n",
    "y = LabeledTweets['Sentiment']\n",
    "\n",
    "# split train test data by 75% train, 25% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    tfidf_matrix, y, test_size=0.25, random_state=42, stratify=y  # set a seed 42 for consistent results for each run \n",
    ")                                                                 # stratify = y ensures class distribution in train and test sets is same as original data.\n",
    "\n",
    "# initialize training on logistic regression\n",
    "model3 = LogisticRegression(max_iter=1000)  # max_iter increased to ensure convergence\n",
    "model3.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred = model3.predict(X_test)\n",
    "# accuracy\n",
    "model3_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Model 3 with feature = 500, Accuracy: {model3_accuracy:.4f}\\n')\n",
    "\n",
    "# model 4, feature = 5000\n",
    "# initialize the vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words = 'english')\n",
    "# fit and transform the cleaned tweets\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(LabeledTweets['clean_tweet'])\n",
    "# define y\n",
    "y = LabeledTweets['Sentiment']\n",
    "\n",
    "# split train test data by 75% train, 25% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    tfidf_matrix, y, test_size=0.25, random_state=42, stratify=y  # set a seed 42 for consistent results for each run \n",
    ")                                                                 # stratify = y ensures class distribution in train and test sets is same as original data.\n",
    "\n",
    "# initialize training on logistic regression\n",
    "model4 = LogisticRegression(max_iter=1000)  # max_iter increased to ensure convergence\n",
    "model4.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred = model4.predict(X_test)\n",
    "# accuracy\n",
    "model4_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Model 4 with feature = 5000, Accuracy: {model4_accuracy:.4f}\\n')\n",
    "\n",
    "# model 5, feature = 50000\n",
    "# initialize the vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=50000, stop_words = 'english')\n",
    "# fit and transform the cleaned tweets\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(LabeledTweets['clean_tweet'])\n",
    "# define y\n",
    "y = LabeledTweets['Sentiment']\n",
    "\n",
    "# split train test data by 75% train, 25% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    tfidf_matrix, y, test_size=0.25, random_state=42, stratify=y  # set a seed 42 for consistent results for each run \n",
    ")                                                                 # stratify = y ensures class distribution in train and test sets is same as original data.\n",
    "\n",
    "# initialize training on logistic regression\n",
    "model5 = LogisticRegression(max_iter=1000)  # max_iter increased to ensure convergence\n",
    "model5.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred = model5.predict(X_test)\n",
    "# accuracy\n",
    "model5_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Model 5 with feature = 50000, Accuracy: {model5_accuracy:.4f}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "Model 1 with feature = 50, Accuracy: 0.8861\n",
    "\n",
    "Model 2 with feature = 5, Accuracy: 0.7413\n",
    "\n",
    "Model 3 with feature = 500, Accuracy: 0.8931\n",
    "\n",
    "Model 4 with feature = 5000, Accuracy: 0.8691\n",
    "\n",
    "Model 5 with feature = 50000, Accuracy: 0.8691\n",
    "\n",
    "It looks like the accuracy improves with features from 5 to 50 to 500, but after features go over 500, it drops at features = 5000, and features = 50000. Therefore for a model with features = 500 is a a better fit than having too few or too many features in terms of model prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
